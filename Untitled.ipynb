{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d77de6c0-3b57-4051-8cf8-8d2aa8b81f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "import urllib.parse\n",
    "import json\n",
    "import re\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "import emoji\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import FreqDist\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62068bac-e80c-44f3-8314-4c8ea1d5c9fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/03/15 14:03:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/03/15 14:03:46 WARN ExecutorAllocationManager: Dynamic allocation without a shuffle service is an experimental feature.\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "from pyspark.sql import SparkSession\n",
    "from operator import add\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# New API\n",
    "spark_session = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"test_notebook\")\\\n",
    "    .master(\"spark://host-192-168-2-176-de1:7077\")\\\n",
    "    .config(\"spark.dynamicAllocation.enabled\", True)\\\n",
    "    .config(\"spark.dynamicAllocation.shuffleTracking.enabled\", True)\\\n",
    "    .config(\"spark.shuffle.service.enabled\", False)\\\n",
    "    .config(\"spark.dynamicAllocation.executorIdleTimeout\",\"100s\")\\\n",
    "    .config(\"spark.driver.port\",9998)\\\n",
    "    .config(\"spark.blockManager.port\",10005)\\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Old API (RDD)\n",
    "spark_context = spark_session.sparkContext\n",
    "\n",
    "spark_context.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9aeb0e9c-d83e-4e95-b285-251ecb3d0b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "      StructField(\"subreddit\",StringType(),True),\n",
    "      StructField(\"body\",StringType(),True)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2571b3a9-6617-4261-8d9d-88d862d510f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark_session.read.schema(schema).json(\"hdfs://host-192-168-2-176-de1:9000/comments/RC_2011-07\")\n",
    "#write to parquet ( done only once)\n",
    "#df.write.parquet(\"hdfs://host-192-168-2-176-de1:9000/comments/parquet/reddit.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "157b6c9c-443b-452a-a918-7079442d6ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "parqDF = spark_session.read.parquet(\"hdfs://host-192-168-2-176-de1:9000/comments/parquet/reddit.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db1ad61f-0155-4b6a-b191-c587e02f6d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:====================================================>    (11 + 1) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the operation takes 5.538941860198975 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/15 14:46:32 ERROR StandaloneSchedulerBackend: Application has been killed. Reason: Master removed our application: KILLED\n",
      "22/03/15 14:46:32 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exiting due to error from cluster scheduler: Master removed our application: KILLED\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.error(TaskSchedulerImpl.scala:919)\n",
      "\tat org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend.dead(StandaloneSchedulerBackend.scala:154)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint.markDead(StandaloneAppClient.scala:262)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anonfun$receive$1.applyOrElse(StandaloneAppClient.scala:169)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "most_popular_categories = parqDF.groupBy('subreddit').count().sort('count', ascending=False).head(50)\n",
    "end = time.time()\n",
    "print(\"the operation takes {0} seconds\".format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d64c0a43-a7db-4f7a-b535-1ea55083521e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:=============================>                             (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the operation takes 63.917561769485474 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "most_popular_categories = df.groupBy('subreddit').count().sort('count', ascending=False).head(50)\n",
    "end = time.time()\n",
    "print(\"the operation takes {0} seconds\".format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708cf9d3-5ab6-40b4-b9e0-d7b677cf30f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3ca6e3-e8fa-41c7-9ccb-1340d511ba87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea4f3ca-e745-466a-8f56-ef46bf4f1f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_session.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
